{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# L1 - Градиентый спуск и линейные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. Линейные модели\n",
    "\n",
    "Пусть есть обучающая выборка $\\{x_i\\}_{i=1}^{\\mathcal{l}} \\subset \\mathbb{R}^{n}$, при этом каждому объекту в соответствие поставлена метка класса $y_{i} \\in \\{-1, +1\\}$. Мы предполагаем, что в пространстве $\\mathbb{R}^{n}$ существует гиперплоскость, которая относительно некоторой метрики \"хорошо\" разделяет объекты на два класса. При этом гиперплоскость задается параметрически:\n",
    "\n",
    "$$wx + b = 0$$\n",
    "\n",
    "Объект $x$ имеет метку $y = +1$, если $wx + b \\geq 0$ и $y = -1$ в ином случае. Вектор $w$ является нормалью к гиперплоскости, указывающей с какой стороны находятся объекты класса $y = +1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Обучение\n",
    "\n",
    "Поиск модели ограничен до одного семейства, заданного параметрически. Обучение в таком случае сводится к задаче оптимизации эмпирического риска\n",
    "\n",
    "\n",
    "$$\\arg\\min_{\\theta} Q(\\theta) = \\arg\\min_{\\theta} \\frac{1}{l}\\sum_{i=1}^{\\mathcal{l}} \\mathcal{L}(a(x_i|\\theta), y_i)$$\n",
    "\n",
    "* $a(x|\\theta)$ - алгоритм из некотрого семейства, заданный параметром $\\theta$\n",
    "* $\\theta$ - вектор пространства параметров\n",
    "* $\\mathcal{L}$ - функция потерь, которая показывает насколько точно предсказание\n",
    "\n",
    "Очевидно, что качество предсказания зависит от выбранной модели. Но также оно зависит и от выбора функции потерь $\\mathcal{L}$, которая существенно влияет на процесс обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Функция потерь\n",
    "\n",
    "В литературе можно встретить такое понятие, как отступ\n",
    "$$M(x, y) = y\\cdot(wx + b),$$\n",
    "его можно трактовать, как уровень удаления от гиперплоскости в сторону своего класса. Это позволит нам кратко записывать функции потерь.\n",
    "\n",
    "Наиболее естественной функцией потерь для задачи классификации является относительное количество неправильных классификаций, то есть\n",
    "$$ \\mathcal{L}(y_{pred}, y_{true}) = [y_{pred} \\neq y_{true}] = [M(x, y_{true}) < 0].$$\n",
    "\n",
    "Решение такой задачи является очень трудоемким, поэтому на практике производят оптимизацию реклаксированной ошибки.\n",
    "\n",
    "К примеру **квадратичная ошибка**\n",
    "\n",
    "$$ Q(w) = \\frac{1}{\\mathcal{l}} \\sum_{i=1}^{\\mathcal{l}}((wx_i+b) - y_i)^{2}.$$\n",
    "\n",
    "Она многим хороша, к примеру, в задаче оптимизации всё сводится к выпуклому функционалу с локальным минимумом. Если представить, что признаки объекта $x_i$ записаны в матрицу $X$ построчно, а все метки записаны в вектор-столбец $Y$, то задача выглядит как\n",
    "$$\\arg\\min_{w}||Xw - Y ||_{2},$$\n",
    "и имеет аналитическое решение\n",
    "$$w = (X^TX)^{-1}X^TY.$$\n",
    "\n",
    "В этой задаче оптимизации отсутствует $b$. Чтобы учесть его, достаточно добавить столбец единиц справа для матрицы $X$. Тогда последнее значение вектора $w$ и будет $b$.\n",
    "\n",
    "**Задание**\n",
    "\n",
    "1. Сгенерируйте на плоскости 2 облака точек. Они должны слегка пересекаться, а точки внутри распределены нормально.\n",
    "2. Обучите линейную модель, разделяющую два облака точек, использую формулу выше.\n",
    "3. Изобразите облака с помощью библиотеки matplotlib, воспользуйтесь функцией scatter, для удобства точки можно сделать прозрачными.\n",
    "4. Постройте полученнную разделяющую прямую.\n",
    "5. Оцените сложность алгоритма обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ещё популярна следующая релаксация\n",
    "$$Q(w) = \\frac{1}{\\mathcal{l}} \\sum_{i=1}^{\\mathcal{l}} max(0, 1 - y_i\\cdot(wx_i + b)),$$\n",
    "если хотите узнать об этом более подробно, то вам стоит почитать про svm (support vector machine).\n",
    "\n",
    "Логистическая функция же обладает вероятностным смыслом\n",
    "\n",
    "$$ Q(w) = \\frac{1}{\\mathcal{l}} \\sum_{i=1}^{\\mathcal{l}} \\ln(1 + \\exp(-y_i\\cdot(wx_i + b)))$$\n",
    "В частности, данный функционал приводит нас к оптимальному байесовскому классификатору при некоторых допущениях о распределении признаков. Но это совершенно отдельная история.\n",
    "\n",
    "**Задание**\n",
    "\n",
    "1. Пусть $\\mathbb{P}\\{y=1|x\\} = \\sigma(wx+b)$, где $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$. Покажите, что задача\n",
    "$$ \\arg\\min_{w, b} \\sum_{x, y} \\ln(1 + \\exp(-y(wx + b)))$$\n",
    "есть не что иное, как максимизация правдоподобия.\n",
    "2. Отобразите все функционалы качества в осях $M \\times Q$ для одного элемента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4. Градиентный спуск\n",
    "\n",
    "Для задачи оптимизации не всегда существует аналитическое решение, либо оно может быть очень сложным. В таком случае используют численные методы. Да, речь идет именно о градиентном спуске. Это итеративный алгоритм, который устроен следующим образом. Пусть есть $Q(x)$, которую необходимо оптимизировать и она дифференцируема. Тогда задачу\n",
    "\n",
    "$$\\arg\\min_{x} Q(x)$$\n",
    "\n",
    "можно решить следующим образом\n",
    "\n",
    "$$ x^{k+1} = x^{k} - \\lambda \\cdot \\triangledown Q(x),$$\n",
    "\n",
    "где $\\lambda$ - некоторый шаг градиентного спуска, а $k$ - номер этого шага.\n",
    "\n",
    "От выбора правильного $\\lambda$ сильно зависит процесс обучения. Если взять слишком большое значение, то алгоритм может не сойтись. Если слишком малое, то обучение будет длиться долго. Также существует распространенный прием, часто применяемый при обучении нейросетей: уменьшать значение $\\lambda$ в соответствии с некоторым расписанием.\n",
    "\n",
    "**Кратко о сходимости**\n",
    "1. Оптимизируем выпуклую функцию\n",
    "2. $\\lambda(t) \\rightarrow 0$\n",
    "3. $\\sum_t \\lambda(t) = \\infty$\n",
    "4. $\\sum_t \\lambda^2(t) < \\infty$\n",
    "\n",
    "**Задание**\n",
    "1. Предложите какую-нибудь квадратичную функцию с глобальным минимумом.\n",
    "2. Найдите минимум методом градиентного спуска.\n",
    "3. Отобразите на плоскости линии уровней функции, которую вы оптимизируете.\n",
    "4. Покажите, какую траекторию проходит алгоритм градиентного спуска.\n",
    "5. Как вы выбрали значение $\\lambda$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Существуют функции, которые плохо даются градиентному спуску. К примеру, функция Розенброка\n",
    "\n",
    "$$f(x, y) = (1-x)^2 + 100(y-x^2)^2.$$\n",
    "\n",
    "**Задание**\n",
    "1. Проделайте все то же самое для функции Розенброка.\n",
    "2. Какую проблему вы наблюдаете?\n",
    "3. Как ее можно решить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Существуют различные модификации алгоритма градиентного спуска. К примеру, метод наискорейшего спуска, где значение $\\lambda$ зависит от шага\n",
    "\n",
    "$$\\lambda^{k} = \\arg\\min_{\\lambda}Q(x_k - \\lambda\\triangledown Q(x_k)).$$\n",
    "\n",
    "**Задание**\n",
    "1. Снова разделите облака точек, только теперь оптимизируйте квадратичную ошибку методом градиентного спуска.\n",
    "2. Отобразите полученную прямую и облака точек.\n",
    "3. Сравните ответ с точным решением.\n",
    "4. Попробуйте метод наискорейшего спуска.\n",
    "5. Постройте график в осях (номер шага и значение $Q$).\n",
    "6. Сравните скорость сходимости обычного и наискорейшего спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "И еще немного о проблемах градиентного спуска. Если у нас есть какие-то признаки, которые встречаются достаточно редко, то соответствующий столбец будет разреженным.\n",
    "\n",
    "**Задание**\n",
    "В чем заключается проблема?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Также нужно понимать, что градиентный спуск может попасть в \"ловушку\" локального минимума. Обычно это актуально для нейросетей. Самый простой способо решить эту проблема - сдедать несколько запусков алгоритма или иметь какой-то инсайт, из какой точки стоит начинать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5. Стохастический градиентный спуск\n",
    "\n",
    "Иногда количество данных может быть так велико, что даже градиентный спуск начинает работать медленно. Или же данные просто поступают к нам большим потоком, а параметры модели постепенно меняются. Тогда на сцену выходит метод стохастического градиента.\n",
    "\n",
    "Идея предельно проста. Можно делать шаг спуска, вычисляя ошибку и градиент не для всех элементов выборки, а для какого-то небольшого количества или даже для одного объекта.\n",
    "\n",
    "**Задание**\n",
    "\n",
    "1. Скачайте данные mnist c [Kaggle](https://www.kaggle.com/c/digit-recognizer).\n",
    "2. Обучите линейный классификатор 0 и 1, используйте логистическую функцию потерь.\n",
    "3. Проверьте качество классификации на отложенной выборке.\n",
    "$$\\mathcal{L}(y_{pred}, y_{true}) = [y_{pred} \\neq y_{true}]$$\n",
    "4. Как влияет размер батча на скорость и качество обучения?\n",
    "5. Отобразите графики, которые доказывает ваши слова (оси придумайте сами).\n",
    "6. Сколько проходов по данным вы делаете? Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "У стохастического градиентного спуска также есть много всяких усовершествований, которые часто используются в реальной практике при обучении нейросетей.\n",
    "\n",
    "Кстати, текущее значение $Q$ можно вычислять с помощью экспоненциального сглаживания.\n",
    "$$Q^{k+1} = \\gamma Q^k + (1 - \\gamma) Q(x_{k+1}),$$\n",
    "\n",
    "где $Q(x_{k+1})$ вычисляется для обрабатываемого батча.\n",
    "\n",
    "**Задание**\n",
    "1. Как зависит график от $\\gamma$?\n",
    "2. Каким способом лучше вычислять $Q$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Сохранение импульса**\n",
    "\n",
    "Сохранение импульса позволяет избежать нам осциляции вдоль оси, по которой функция изменяется сильнее. Он заключается в том, что текущий градиентный шаг вычисляется на основе учета предыдущих шагов\n",
    "$$x^{k+1} = x^{k} - s^{k},$$ где $s^k = \\gamma s^{k-1} + \\lambda\\triangledown Q(x^k)$, при этом\n",
    " * $0 <\\gamma < 1$ - коэффициент учета предыдущего импульса\n",
    " * $s^{-1} = 0$\n",
    " \n",
    "**Задание**\n",
    "\n",
    "1. Найдите минимум $Q(x, y) = 10x^2 + y^2$ c помощью обычного метода.\n",
    "2. Воспользуйтесь методом сохранения импульса\n",
    "3. Отобразите и сравните треки.\n",
    "4. На основе чего вы выбрали $\\gamma$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Ускоренный градиент Нестерова**\n",
    "\n",
    "И логическое развитие этого подхода приводит к методу ускоренного градиента Нестерова. Шаг спуска вычисляется немного иначе\n",
    "$$s^k = \\gamma s^{k-1} + \\lambda\\triangledown Q(x^k - \\gamma s^{k-1}),$$\n",
    "то есть мы вычисляем градиент фукнции примерно в той точке, куда \"занесет\" нас накопленный импульс.\n",
    "\n",
    "**Задание**\n",
    "\n",
    "1. Сравните этот метод и предыдущий на функции Розенброка.\n",
    "2. Отобразите и сравните треки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для дополнительного чтения**\n",
    "1. Adagrad (2011)\n",
    "2. RMSprop\n",
    "3. Adadelta (2012)\n",
    "4. Adam (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Многоклассовая классификация\n",
    "\n",
    "Итак, мы знакомы уже с бинарным линейным классификатором\n",
    "$$y = \\text{sign}(wx).$$\n",
    "Существуют [разные](https://en.wikipedia.org/wiki/Multiclass_classification) подходы к задаче многоклассовой классификции, к примеру [сведение](https://en.wikipedia.org/wiki/Multiclass_classification#Transformation_to_Binary) задачи к бинарной классификации, [модификация модели](https://en.wikipedia.org/wiki/Support_vector_machine#Multiclass_SVM) и т.п. Нам же интересен подход, который применяется в нейронных сетях.\n",
    "\n",
    "Для каждого класса из набора $1 \\dots |C|$ заведем свой вектор весов $w_i$ и уложим это все в матрицу $W$ построчно. Для простоты будем считать, что $w_i$ - строка. Тогда наш классификатор будет выглядеть следующим образом\n",
    "$$(p_1, \\dots, p_{|C|}) = \\text{softmax}(Wx),$$\n",
    "где $p_i$ - вероятность, что объект относится к классу $i$, при этом\n",
    "$$p_i = \\frac{\\exp(w_ix)}{\\sum_j \\exp(w_jx)}.$$\n",
    "Если внимательно присмотреться, то $\\text{softmax}$ является обобщенным вариантом сигмоиды. Для того, чтобы убедиться в этом, достаточно расписать случай для $|C|=2$.\n",
    "\n",
    "Как и для задачи бинарной классификации, обучение можно свести к минимизации эмпирического риска, то есть к оптимизации следующего функционала\n",
    "$$\\arg\\min_W Q(W) = \\arg\\min_W -\\frac{1}{\\mathcal{l}}\\sum_y\\sum_i [y = i] \\cdot \\ln(p_i(W)).$$\n",
    "Очевидно, что сверху написано ни что иное, как максимизация логарифма правдоподобия.\n",
    "\n",
    "**Задание**\n",
    "1. Вычислите градиент функции $Q$ (попробуйте провести выкладки для отдельной строки $w_i$).\n",
    "2. Обучите модель с помощью градиентного спуска на выборке [mnist](https://www.kaggle.com/c/digit-recognizer) (вы можете применить свою любимую вариацию метода).\n",
    "3. Вычислите качество на отоженной выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Регуляризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
